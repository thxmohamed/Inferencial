---
title: "Lectura15"
author: "Benjamín Bustamante y Mohamed Al-Marzuk"
date: "`r Sys.Date()`"
output: html_document
---

REGRESIÓN LOGÍSTICA

Esta usa como función de enlace la función logit, cuya inversa es la función logística estándar.

Evaluación de un clasificador

- **Verdaderos positivos (VP)**: cantidad de instancias correctamente clasificadas como pertenecientes a la clase positiva.
- **Falsos positivos (FP)**: cantidad de instancias erróneamente clasificadas como pertenecientes a la clase positiva.
- **Falsos negativos (FN)**: cantidad de instancias erróneamente clasificadas como pertenecientes a la clase negativa.
- **Verdaderos negativos (VN)**: cantidad de instancias correctamente clasificadas como pertenecientes a la clase negativa.

## Exactitud

La **exactitud (accuracy)** del clasificador corresponde a la proporción de observaciones correctamente clasificadas, dada por la ecuación:

$$
\text{exactitud} = \frac{VP + VN}{n}
$$

## Error

A su vez, el **error** del clasificador corresponde a la proporción de observaciones clasificadas de manera equivocada:

$$
\text{error} = \frac{FP + FN}{n} = 1 - \text{exactitud}
$$

## Sensibilidad

La **sensibilidad (sensitivity o recall)** indica cuán apto es el clasificador para detectar aquellas observaciones pertenecientes a la clase positiva:

$$
\text{sensibilidad} = \frac{VP}{VP + FN}
$$

## Especificidad

De manera análoga, la **especificidad (specificity)** permite determinar cómo el clasificador es capaz de acertar correctamente asignar observaciones a la clase negativa:

$$
\text{especificidad} = \frac{VN}{FP + VN}
$$

## Precisión

La **precisión (precision)** o valor predictivo positivo (**VPP**) indica cuán exitosa es la asignación de elementos a la clase positiva:

$$
\text{VPP} = \frac{VP}{VP + FP}
$$

## Valor Predictivo Negativo

Asimismo, el **valor predictivo negativo (VPN)** señala la proporción de instancias correctamente clasificadas como pertenecientes a la clase negativa:

$$
\text{VPN} = \frac{VN}{VN + FN}
$$
Otra herramienta es la curva de calibración (ROC), que muestra la relación entre la sensibilidad y la especifidad del modelo. Hay que ver el alejamiento de la diagonal, que es conocido como AUC, que varía entre 0 y 1, y es el área bajo la curva ROC. Un AUC más alto indica un mejor desempeño del modelo en la clasificación. Un AUC de 1 exactamente, es un clasificador perfecto. Uno que no discrimina, es decir, su desempeño no es mejor que el de una clasificación aleatoria, se asocia a un AUC = 0.5.

REGRESIÓN LOGÍSTICA EN R

En R, la llamada glm(formula, family = binomial(link = "logit"), data) permite ajustar un modelo de regresión logística, donde:

* formula: tiene la forma <variable de respuesta>~<variables predictoras>.

* data: matriz de datos.

el argumento family = binomial(link = "logit") indica que asumiremos una distribución binomial para la variable de respuesta y que usaremos logit como enlace.

```{r}
library(caret)
library(dplyr)
library(ggpubr)
library(pROC)

# Cargar y filtrar los datos, teniendo cuidado de dejar
# automático como 2do nivel de la variable "am" para que
# sea considerada como la clase positiva.

datos <- mtcars |> filter(wt > 2 & wt < 5) |>
  mutate(am = factor(am, levels = c(1, 0), labels = c("manual", "automático")))

# Separar conjuntos de entrenamiento y prueba

set.seed(101)
n <- nrow(datos)
i_muestra <- sample.int(n = n, size = floor(0.7 * n), replace = FALSE)

datos_ent <- datos[i_muestra, ]
datos_pru <- datos[-i_muestra, ]

# Ajustar modelo

modelo <- glm(am ~ wt, family = binomial(link = "logit"), data = datos_ent)

print(summary(modelo))

```
Evaluar el modelo con el conjunto de entrenamieto

```{r}

probs_ent <- fitted(modelo)

# Graficar curva ROC, indicando AUC obtenido.

ROC_ent <- roc(datos_ent[["am"]], probs_ent)
texto_ent <- sprintf("AUC = %.2f", ROC_ent[["auc"]])
g_roc_ent <- ggroc(ROC_ent, color = 2)
g_roc_ent <- g_roc_ent + geom_segment(aes(x = 1, rend = 0, y = 0, yend = 1),
                                      linetype = "dashed")
g_roc_ent <- g_roc_ent + annotate("text", x = 0.3, y = 0.3, label = texto_ent)
g_roc_ent <- g_roc_ent + theme_pubr()

print(g_roc_ent)

```

Obtener las predicciones

```{r}
umbral <- 0.5
preds_ent <- sapply(probs_ent,
                    function(p) ifelse(p >= umbral, "automático", "manual"))
preds_ent <- factor(preds_ent, levels = levels(datos[["am"]]))

# Obtener y mostrar estadísticas de clasificación en datos de entrenamiento.

mat_conf_ent <- confusionMatrix(preds_ent, datos_ent[["am"]],
                                positive = "automático")

cat("\n\nEvaluación del modelo (conjunto de entrenamiento):\n")
cat("--------------------------------------------------\n")
print(mat_conf_ent[["table"]])
cat("\n")

cat(sprintf("Exactitud:     %.3f\n", mat_conf_ent[["overall"]]["Accuracy"]))
cat(sprintf("Sensibilidad:  %.3f\n", mat_conf_ent[["byClass"]]["Sensitivity"]))
cat(sprintf("Especificidad: %.3f\n", mat_conf_ent[["byClass"]]["Specificity"]))

```
Evaluar el modelo con el conjunto de prueba

```{r}

probs_pru <- predict(modelo, datos_pru, type = "response")

# Graficar curva ROC, indicando AUC obtenido.

ROC_pru <- roc(datos_pru[["am"]], probs_pru)
texto_pru <- sprintf("AUC = %.2f", ROC_pru[["auc"]])
g_roc_pru <- ggroc(ROC_pru, color = 2)

g_roc_pru <- g_roc_pru + geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1),
                                      linetype = "dashed")

g_roc_pru <- g_roc_pru + annotate("text", x = 0.3, y = 0.3, label = texto_pru)

g_roc_pru <- g_roc_pru + theme_pubr()

print(g_roc_pru)

```

Ahora procedemos a obtener las predicciones (con el mismo umbral)

```{r}
preds_pru <- sapply(probs_pru,
                    function(p) ifelse(p >= umbral, "automático", "manual"))

preds_pru <- factor(preds_pru, levels = levels(datos[["am"]]))

# Obtener y mostrar estadísticas de clasificación en datos de prueba.

mat_conf_pru <- confusionMatrix(preds_pru, datos_pru[["am"]],
                                positive = "automático")

cat("\n\nEvaluación del modelo (conjunto de entrenamiento):\n")
cat("--------------------------------------------------\n")
print(mat_conf_pru[["table"]])
cat("\n")

cat(sprintf("Exactitud:     %.3f\n", mat_conf_pru[["overall"]]["Accuracy"]))
cat(sprintf("Sensibilidad:  %.3f\n", mat_conf_pru[["byClass"]]["Sensitivity"]))
cat(sprintf("Especificidad: %.3f\n", mat_conf_pru[["byClass"]]["Specificity"]))


```
Ajuste de un modelo de regresión logísitca usando validación cruzada

```{r}
library(caret)
library(data.table)
library(dplyr)

# Cargar y filtrar los datos, teniendo cuidado de dejar
# autómatico como 2do nivel de la variable "am" para que
# sea considerada como la clase positiva.

datos <- mtcars |> filter(wt > 2 & wt < 5) |>
  mutate(am = factor(am, levels = c(1, 0), labels = c("manual", "automático")))

# Separar conjuntos de entrenamiento y prueba.

set.seed(101)

n <- nrow(datos)

i_muestra <- sample.int(n = n, size = floor(0.7 * n), replace = FALSE)

datos_ent <- datos[i_muestra, ]
datos_pru <- datos[-i_muestra, ]

# Ajustar modelo usando validación cruzada de la 4 pliegues.

modelo_ent <- train(am ~ wt, data = datos_ent, method = "glm",
                    family = binomial(link = "logit"),
                    trControl = trainControl(method = "cv", number = 4,
                                             savePredictions = TRUE))

modelo <- modelo_ent[["finalModel"]]

cat("\nModelo RLog :\n")
cat("-------------------\n")
print(summary(modelo))

```

Obtener y mostrar estadísticas de clasificación en datos de entrenamiento.

```{r}

mat_conf_ent <- confusionMatrix(modelo_ent[["pred"]][["pred"]],
                                modelo_ent[["pred"]][["obs"]],
                                positive = "automático")

cat("\nEvaluación del modelo (conjunto de entrenamiento):\n")
cat("------------------------------------------------------")
print(mat_conf_ent[["table"]])

cat("\n")

cat(sprintf("Exactitud:     %.3f\n", mat_conf_ent[["overall"]]["Accuracy"]))
cat(sprintf("Sensibilidad:  %.3f\n", mat_conf_ent[["byClass"]]["Sensitivity"]))
cat(sprintf("Especificidad: %.3f\n", mat_conf_ent[["byClass"]]["Specificity"]))

```
Procedemos a ver los detalles por cada pliegue

```{r}
cat("\n\nDetalle por pliegue:\n")
cat("-------------------------------\n")

resumen <- data.table(modelo_ent[["resample"]][, c(1, 3)])

resumen <- rbind(resumen, list(modelo_ent[["results"]][[2]], "Mean"))
resumen <- rbind(resumen, list(modelo_ent[["results"]][[4]], "SD"))

print(resumen[1:4, ], row.names = FALSE)
cat("---------------------------\n")
print(resumen[5:6, ], row.names = FALSE, col.names = "none", digits = 3)

# Obtener las predicciones en los datos de prueba.

umbral <- 0.5
probs <- predict(modelo, datos_pru, type = "response")
preds <- ifelse(probs >= umbral, "automático", "manual")
preds <- factor(preds, levels = levels(datos[["am"]]))

# Obtener y mostrar estadísticas de clasificación en datos de entrenamiento.

mat_conf_pru <- confusionMatrix(preds, datos_pru[["am"]],
                                positive = "automático")

cat("\nEvaluación del modelo (conjunto de entrenamiento):\n")
cat("------------------------------------------------------")
print(mat_conf_pru[["table"]])

cat("\n")

cat(sprintf("Exactitud:     %.3f\n", mat_conf_pru[["overall"]]["Accuracy"]))
cat(sprintf("Sensibilidad:  %.3f\n", mat_conf_pru[["byClass"]]["Sensitivity"]))
cat(sprintf("Especificidad: %.3f\n", mat_conf_pru[["byClass"]]["Specificity"]))

```

REGRESIÓN CON MÚLTIPLES PREDICTORES

Hay que utilizar la regresión jerárquica para escoger los predictores. Hay que hacer selección hacia adelante, eliminación hacia atrás, regresión escalonada o todos los subconjuntos, usando las mismas funciones de R que el capítulo anterior.

```{r}
library(dplyr)
library(ggpubr)

# Cargar y filtrar los datos (solo predictores numéricos)

datos <- mtcars |> filter(wt > 2 & wt < 5) |>
  select (-c("cyl", "vs", "gear", "carb")) |>
  mutate(am = factor(am, levels = c(1, 0), labels = c("manual", "automático")))

# Separar conjuntos de entrenamiento y prueba

set.seed(101)
n <- nrow(datos)
i_muestra <- sample.int(n = n, size = floor(0.7 * n), replace = FALSE)

datos_ent <- datos[i_muestra, ]
datos_pru <- datos[-i_muestra, ]

# Modelos inicial y máximo

nulo <- glm(am ~ 1, family = binomial(link = "logit"), data = datos_ent)
maxi <- glm(am ~ ., family = binomial(link = "logit"), data = datos_ent)

# Ajustar modelo con regresión paso a paso escalonada.

modelo <- step(nulo, scope = list(upper = maxi),
               direction = "both", trace = FALSE)

cat("\nModelo RLog conseguido con regresión escalonada:\n")
cat("--------------------------------------------------")
print(summary(modelo))

```
```{r}
library(dplyr)
library(ggpubr)

# Imprimir mensajes de advertencia a medida que ocurre

opt <- options(warn = 1, width = 26)

# Cargar y filtrar los datos (solo predictores numéricos).

datos <- mtcars |> filter(wt > 2 & wt < 5) |>
  select(-c("cyl", "vs", "gear", "carb")) |>
  mutate(am = factor(am, levels = c(1, 0), labels = c("manual", "automático")))

# Separar conjuntos de entrenamiento y prueba.

set.seed(101)
n <- nrow(datos)
i_muestra <- sample.int(n = n, size = floor(0.7 * n), replace = FALSE)

datos_ent <- datos[i_muestra, ]
datos_pru <- datos[-i_muestra, ]

# Modelos inicial y máximo

nulo <- glm(am ~ 1, family = binomial(link = "logit"), data = datos_ent)
maxi <- glm(am ~ ., family = binomial(link = "logit"), data = datos_ent)

# Revisar un paso hacia adelante.

cat("\nPaso 1:\n")
cat("------------\n")
print(add1(nulo, scope = maxi))

# Actualizar el modelo

modelo1 <- update(nulo, . ~ . + wt)

# Revisar un paso hacia adelante
cat("\nPaso 2:\n")
cat("-------------\n")
print(add1(modelo1, scope = maxi))

# Actualizar el modelo

modelo2 <- update(modelo1, . ~ . + mpg)

# Revisar un paso hacia adelante

cat("\nPaso 3:\n")
cat("--------------\n")
print(add1(modelo2, scope = maxi))

# Mostrar el modelo obtenido

cat("\nModelo RLog conseguido con regresión hacia adelante:\n")
cat("------------------------------------------------\n")
print(summary(modelo2))

# Comparar los modelos generados

cat("Comparación de los modelos considerados:\n")
cat("-------------------------------------------\n")
print(anova(nulo, modelo1, modelo2, test = "LRT"))

# Reestablecer opción para warnings

options(warn = opt[[1]], width = opt[[2]])

```




